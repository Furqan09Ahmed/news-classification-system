{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb428b76",
   "metadata": {},
   "source": [
    "# Phase 2: Text Preprocessing Pipeline\n",
    "**Project:** Document Classification System  \n",
    "**Goal:** Transform raw, noisy news text into clean, lemmatized tokens for Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1863c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furqan/news-document-classifier/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de85206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c86de4",
   "metadata": {},
   "source": [
    "## 1. Load & Version Data\n",
    "We download from Hugging Face but save a local copy to `data/raw` to ensure reproducibility and offline access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9e72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f6609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data saved. Training samples: 120000\n"
     ]
    }
   ],
   "source": [
    "# Save the raw \"Source of Truth\"\n",
    "df_train.to_csv('../data/raw/train.csv', index=False)\n",
    "df_test.to_csv('../data/raw/test.csv', index=False)\n",
    "\n",
    "print(f\"Raw data saved. Training samples: {len(df_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e6d98",
   "metadata": {},
   "source": [
    "## 2. Build the Preprocessing Engine\n",
    "This function handles the \"Four Pillars of Cleaning\":\n",
    "1. **Decoding:** Fixes `#36;` ($), `#151;` (â€”), and `&quot;`.\n",
    "2. **Stripping:** Removes news source headers (e.g., \"Reuters -\").\n",
    "3. **Regex:** Removes punctuation, numbers, and backslashes.\n",
    "4. **Lemmatization:** Reduces words to their base form (e.g., \"running\" -> \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e3f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model (disable parser and ner for speed)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4053d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # 1. Decode HTML\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # 2. BETTER CLEANING: Remove typical news headers (Reuters, AP, etc.)\n",
    "    # This looks for \"(Reuters)\", \"(AP)\", or \"CITY (Reuters) -\" \n",
    "    text = re.sub(r'\\(Reuters\\)', '', text)\n",
    "    text = re.sub(r'\\(AP\\)', '', text)\n",
    "    text = re.sub(r'^[A-Z\\s,]+ \\(Reuters\\) - ', '', text) # e.g. \"NEW YORK (Reuters) - \"\n",
    "    text = re.sub(r'^[A-Z\\s,]+ \\(AP\\) - ', '', text)     # e.g. \"WASHINGTON (AP) - \"\n",
    "\n",
    "    # 3. Replace problematic separators with spaces\n",
    "    text = re.sub(r'[\\\\/_-]', ' ', text)\n",
    "    \n",
    "    # 4. Standard clean\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 5. Lemmatize\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and len(token.text) > 2]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc8ce5",
   "metadata": {},
   "source": [
    "## 3. Execute the Pipeline\n",
    "We apply the `clean_text` function to our training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3876b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing testing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>wall bears claw black reuter short seller wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>carlyle look commercial aerospace reuters priv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>oil economy cloud stock outlook reuter soar cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>iraq halt oil export main southern pipeline re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>oil price soar time record pose new menace eco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "      <td>stock end near year low reuter stock end sligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
       "      <td>money fund fall late week asset nation retail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
       "      <td>fed minute dissent inflation usatodaycom usato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
       "      <td>safety net forbescom forbescom earn phd sociol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black&nbsp;&nbsp;NEW Y...</td>\n",
       "      <td>wall bears claw black new york&nbsp;&nbsp;&nbsp;&nbsp; short selle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...   \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  Oil prices soar to all-time record, posing new...   \n",
       "5  Stocks End Up, But Near Year Lows (Reuters) Re...   \n",
       "6  Money Funds Fell in Latest Week (AP) AP - Asse...   \n",
       "7  Fed minutes show dissent over inflation (USATO...   \n",
       "8  Safety Net (Forbes.com) Forbes.com - After ear...   \n",
       "9  Wall St. Bears Claw Back Into the Black  NEW Y...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  wall bears claw black reuter short seller wall...  \n",
       "1  carlyle look commercial aerospace reuters priv...  \n",
       "2  oil economy cloud stock outlook reuter soar cr...  \n",
       "3  iraq halt oil export main southern pipeline re...  \n",
       "4  oil price soar time record pose new menace eco...  \n",
       "5  stock end near year low reuter stock end sligh...  \n",
       "6  money fund fall late week asset nation retail ...  \n",
       "7  fed minute dissent inflation usatodaycom usato...  \n",
       "8  safety net forbescom forbescom earn phd sociol...  \n",
       "9  wall bears claw black new york     short selle...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "df_train['cleaned_text'] = df_train['text'].apply(clean_text)\n",
    "\n",
    "print(\"Processing testing data...\")\n",
    "df_test['cleaned_text'] = df_test['text'].apply(clean_text)\n",
    "\n",
    "# Check a sample to verify #36; and backslashes are gone\n",
    "df_train[['text', 'cleaned_text']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda67b6",
   "metadata": {},
   "source": [
    "## 4. Save Processed \"Gold\" Data\n",
    "We save only the necessary columns to save space. These files will be used for all future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "561e912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Processed data saved to data/processed/\n"
     ]
    }
   ],
   "source": [
    "df_train[['label', 'cleaned_text']].to_csv('../data/processed/news_clean_train.csv', index=False)\n",
    "df_test[['label', 'cleaned_text']].to_csv('../data/processed/news_clean_test.csv', index=False)\n",
    "\n",
    "print(\"Success! Processed data saved to data/processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e56ca",
   "metadata": {},
   "source": [
    "## 5 Summary & Next Steps\n",
    "\n",
    "A robust text preprocessing pipeline was successfully implemented, transforming raw news descriptions into standardized tokens. Techniques including HTML decoding, regex-based cleaning, and spaCy lemmatization were utilized to reduce noise while preserving semantic integrity. The resulting \"Gold\" datasets were exported to the data/processed/ directory.\n",
    "\n",
    "**Next Phase:** The project moves to Traditional Machine Learning Models. TF-IDF vectorization and classical algorithms (Logistic Regression and SVM) will be applied to establish a performance benchmark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
